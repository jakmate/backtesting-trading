{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c8bcbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas_market_calendars as mcal\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "from pytz import timezone\n",
    "import time\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ccbb96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MARKET_TZ = timezone('America/New_York')\n",
    "\n",
    "def is_market_open(date):\n",
    "    nyse = mcal.get_calendar('NYSE')\n",
    "    sched = nyse.schedule(start_date=date, end_date=date)\n",
    "    return not sched.empty\n",
    "\n",
    "def get_next_trading_day(date):\n",
    "    d = date\n",
    "    while True:\n",
    "        d += timedelta(days=1)\n",
    "        if is_market_open(d):\n",
    "            return d\n",
    "\n",
    "def get_previous_trading_day(date):\n",
    "    d = date\n",
    "    while True:\n",
    "        d -= timedelta(days=1)\n",
    "        if is_market_open(d):\n",
    "            return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data(ticker, start_date, end_date, interval, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Retrieve historical data with retry logic for rate limits.\n",
    "    Returns timezone-aware DataFrame in MARKET_TZ.\n",
    "    \"\"\"\n",
    "    os.makedirs('historical_data', exist_ok=True)\n",
    "    file_path = f'historical_data/{ticker}.csv'\n",
    "    \n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            new_data = yf.download(ticker, \n",
    "                                 start=start_date,\n",
    "                                 end=end_date,\n",
    "                                 interval=interval,\n",
    "                                 progress=False)\n",
    "            \n",
    "            if not new_data.empty:\n",
    "                # Process columns and timezone\n",
    "                if isinstance(new_data.columns, pd.MultiIndex):\n",
    "                    new_data.columns = new_data.columns.get_level_values(0)\n",
    "                    \n",
    "                if new_data.index.tzinfo is None:\n",
    "                    new_data.index = new_data.index.tz_localize(MARKET_TZ)\n",
    "                else:\n",
    "                    new_data.index = new_data.index.tz_convert(MARKET_TZ)\n",
    "                \n",
    "                return new_data\n",
    "            \n",
    "            print(f\"Fetched data for {ticker} from {start_date} to {end_date}: {len(new_data)} rows\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                wait_time = backoff_factor ** attempt\n",
    "                print(f\"Retry {attempt+1}/{max_retries} for {ticker} in {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed to fetch {ticker} after {max_retries} retries: {str(e)}\")\n",
    "                return pd.DataFrame()\n",
    "    \n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a0a9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_metrics(ticker):\n",
    "    \"\"\"Fetch fundamental data for a ticker\"\"\"\n",
    "    try:\n",
    "        tk = yf.Ticker(ticker)\n",
    "        info = tk.info\n",
    "        \n",
    "        # Basic metrics\n",
    "        metrics = {\n",
    "            'Symbol': ticker,\n",
    "            'Sector': info.get('sector', 'Unknown'),\n",
    "            'Industry': info.get('industry', 'Unknown'),\n",
    "            'MarketCap': info.get('marketCap', None),\n",
    "            'Beta': info.get('beta', None),\n",
    "            'PE': info.get('trailingPE', None),\n",
    "            'ForwardPE': info.get('forwardPE', None),\n",
    "            'PEG': info.get('pegRatio', None),\n",
    "            'ShortPercentFloat': info.get('shortPercentOfFloat', None),\n",
    "            'AvgVolume': info.get('averageVolume', None),\n",
    "        }\n",
    "        \n",
    "        # Calculate market cap category\n",
    "        mcap = metrics['MarketCap']\n",
    "        if mcap:\n",
    "            if mcap >= 200e9:\n",
    "                metrics['CapSize'] = 'Mega'\n",
    "            elif mcap >= 10e9:\n",
    "                metrics['CapSize'] = 'Large'\n",
    "            elif mcap >= 2e9:\n",
    "                metrics['CapSize'] = 'Mid'\n",
    "            elif mcap >= 300e6:\n",
    "                metrics['CapSize'] = 'Small'\n",
    "            else:\n",
    "                metrics['CapSize'] = 'Micro'\n",
    "        else:\n",
    "            metrics['CapSize'] = 'Unknown'\n",
    "            \n",
    "        return metrics\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching metrics for {ticker}: {str(e)}\")\n",
    "        return {'Symbol': ticker, 'Sector': 'Unknown', 'Industry': 'Unknown', \n",
    "                'CapSize': 'Unknown', 'MarketCap': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c644e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor Extraction via PCA & SVD\n",
    "def extract_factors(returns_df, n_factors=3, denoise=True):\n",
    "    \"\"\"\n",
    "    Extract principal factors from asset returns\n",
    "    \n",
    "    Parameters:\n",
    "    - returns_df: DataFrame of asset returns (rows=time, columns=assets)\n",
    "    - n_factors: Number of factors to retain\n",
    "    - denoise: Whether to apply SVD denoising\n",
    "    \n",
    "    Returns:\n",
    "    - factors_df: DataFrame of factor returns\n",
    "    - loadings_df: DataFrame of factor loadings\n",
    "    - eigenvalues: Array of eigenvalues (variance explained by each factor)\n",
    "    \"\"\"\n",
    "    # Check for sufficient data\n",
    "    if returns_df.shape[0] < returns_df.shape[1]:\n",
    "        print(f\"Warning: More assets ({returns_df.shape[1]}) than time periods ({returns_df.shape[0]}). Results may be unstable.\")\n",
    "    \n",
    "    # Get asset returns as numpy array\n",
    "    returns = returns_df.values\n",
    "    \n",
    "    # 1. PCA approach\n",
    "    # Compute covariance matrix\n",
    "    cov_matrix = np.cov(returns.T)\n",
    "    \n",
    "    # Eigendecomposition\n",
    "    eigenvalues, eigenvectors = linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Sort in descending order (largest eigenvalues first)\n",
    "    idx = eigenvalues.argsort()[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "    \n",
    "    # Keep only top n_factors\n",
    "    eigenvalues = eigenvalues[:n_factors]\n",
    "    eigenvectors = eigenvectors[:, :n_factors]\n",
    "    \n",
    "    # 2. SVD approach for denoising\n",
    "    if denoise:\n",
    "        U, S, Vt = linalg.svd(returns, full_matrices=False)\n",
    "        \n",
    "        # Truncate to keep only top n_factors singular values\n",
    "        S_trunc = np.zeros_like(S)\n",
    "        S_trunc[:n_factors] = S[:n_factors]\n",
    "        \n",
    "        # Reconstruct denoised returns\n",
    "        returns_denoised = U @ np.diag(S_trunc) @ Vt\n",
    "        \n",
    "        # Update covariance and eigenvectors based on denoised returns\n",
    "        cov_matrix_denoised = np.cov(returns_denoised.T)\n",
    "        eigenvalues_denoised, eigenvectors_denoised = linalg.eigh(cov_matrix_denoised)\n",
    "        \n",
    "        # Sort in descending order \n",
    "        idx = eigenvalues_denoised.argsort()[::-1]\n",
    "        eigenvalues_denoised = eigenvalues_denoised[idx]\n",
    "        eigenvectors_denoised = eigenvectors_denoised[:, idx]\n",
    "        \n",
    "        # Update with denoised results\n",
    "        eigenvalues = eigenvalues_denoised[:n_factors]\n",
    "        eigenvectors = eigenvectors_denoised[:, :n_factors]\n",
    "        \n",
    "    # Calculate factor returns: F = R * V\n",
    "    factor_returns = returns @ eigenvectors\n",
    "    \n",
    "    # Create DataFrames\n",
    "    factors_df = pd.DataFrame(\n",
    "        factor_returns, \n",
    "        index=returns_df.index,\n",
    "        columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    \n",
    "    loadings_df = pd.DataFrame(\n",
    "        eigenvectors,\n",
    "        index=returns_df.columns,\n",
    "        columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    "    )\n",
    "    \n",
    "    return factors_df, loadings_df, eigenvalues\n",
    "\n",
    "def avg_corr(window):\n",
    "    \"\"\"\n",
    "    Calculate average pairwise correlation in a window of returns\n",
    "    \n",
    "    Parameters:\n",
    "    - window: DataFrame window from rolling operation, with assets as columns\n",
    "    \n",
    "    Returns:\n",
    "    - avg_correlation: Average pairwise correlation\n",
    "    \"\"\"\n",
    "    # Skip if there are too few observations or all NaNs\n",
    "    if len(window) <= 1 or window.isna().all().all():\n",
    "        return np.nan\n",
    "        \n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = window.corr()\n",
    "    \n",
    "    # Extract upper triangle (excluding diagonal)\n",
    "    mask = np.triu(np.ones_like(corr_matrix), k=1).astype(bool)\n",
    "    upper_tri = corr_matrix.where(mask)\n",
    "    \n",
    "    # Calculate mean of correlations (ignoring NaN values)\n",
    "    avg_correlation = upper_tri.stack().mean()\n",
    "    \n",
    "    return avg_correlation\n",
    "\n",
    "# Regime Detection via Perron-Frobenius\n",
    "def detect_market_regime(returns_df, n_regimes=3, lookback=60):\n",
    "    \"\"\"\n",
    "    Detect market regimes using volatility clustering and Perron-Frobenius analysis\n",
    "    \n",
    "    Parameters:\n",
    "    - returns_df: DataFrame of asset returns\n",
    "    - n_regimes: Number of different regimes to identify\n",
    "    - lookback: Number of days to use for rolling statistics\n",
    "    \n",
    "    Returns:\n",
    "    - regime_df: DataFrame with regime indicators\n",
    "    - transition_matrix: Transition probabilities between regimes\n",
    "    - stationary_dist: Stationary distribution (from Perron-Frobenius)\n",
    "    \"\"\"\n",
    "    # Compute market statistics used for regime identification\n",
    "    market_stats = pd.DataFrame(index=returns_df.index)\n",
    "    \n",
    "    # 1. Rolling volatility (key regime indicator)\n",
    "    market_stats['volatility'] = returns_df.std(axis=1).rolling(lookback).std()\n",
    "    \n",
    "    # 2. Cross-sectional correlation (manual calculation instead of rolling apply)\n",
    "    # Calculate rolling correlation manually to avoid apply issues\n",
    "    corr_values = []\n",
    "    for i in range(len(returns_df)):\n",
    "        if i < lookback:\n",
    "            corr_values.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        window = returns_df.iloc[i-lookback:i]\n",
    "        if len(window) <= 1:\n",
    "            corr_values.append(np.nan)\n",
    "            continue\n",
    "            \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = window.corr()\n",
    "        \n",
    "        # Extract upper triangle (excluding diagonal)\n",
    "        mask = np.triu(np.ones_like(corr_matrix), k=1).astype(bool)\n",
    "        upper_tri = corr_matrix.where(mask)\n",
    "        \n",
    "        # Calculate mean of correlations (ignoring NaN values)\n",
    "        avg_correlation = upper_tri.stack().mean()\n",
    "        corr_values.append(avg_correlation)\n",
    "    \n",
    "    market_stats['correlation'] = corr_values\n",
    "    \n",
    "    # 3. Market trend (positive or negative)\n",
    "    market_stats['trend'] = returns_df.mean(axis=1).rolling(lookback).mean()\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    market_stats = market_stats.dropna()\n",
    "    if market_stats.empty:\n",
    "        print(\"Not enough data for regime detection\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(market_stats)\n",
    "    \n",
    "    # Cluster into regimes using KMeans\n",
    "    kmeans = KMeans(n_clusters=n_regimes, random_state=69, n_init=10)\n",
    "    regimes = kmeans.fit_predict(scaled_features)\n",
    "    \n",
    "    # Create regime DataFrame\n",
    "    regime_df = pd.DataFrame({'Regime': regimes}, index=market_stats.index)\n",
    "    \n",
    "    # Calculate regime characteristics\n",
    "    regime_characteristics = pd.DataFrame()\n",
    "    for i in range(n_regimes):\n",
    "        mask = regime_df['Regime'] == i\n",
    "        if mask.sum() > 0:  # Check if we have data for this regime\n",
    "            stats = market_stats[mask].mean()\n",
    "            stats['count'] = mask.sum()\n",
    "            stats['regime'] = i\n",
    "            regime_characteristics = pd.concat([regime_characteristics, pd.DataFrame([stats])])\n",
    "    \n",
    "    # Label regimes based on volatility (highest vol = crisis, lowest = calm)\n",
    "    vol_order = regime_characteristics.sort_values('volatility').index\n",
    "    regime_map = {old: new for new, old in enumerate(vol_order)}\n",
    "    \n",
    "    # Ensure regime values are integers by first handling NaN values\n",
    "    regime_df['Regime'] = regime_df['Regime'].fillna(-1).astype(int)  # Fill NaN with dummy value\n",
    "    regime_df['Regime'] = regime_df['Regime'].map(lambda x: regime_map.get(x, 0))  # Map with default value\n",
    "    \n",
    "    # Build transition matrix (Markov model)\n",
    "    transitions = np.zeros((n_regimes, n_regimes))\n",
    "    \n",
    "    # Ensure we're using integer indices for the matrix and skip NaN values\n",
    "    for i in range(1, len(regime_df)):\n",
    "        prev_regime = int(regime_df['Regime'].iloc[i-1])\n",
    "        curr_regime = int(regime_df['Regime'].iloc[i])\n",
    "        # Add bounds checking to prevent index errors\n",
    "        if 0 <= prev_regime < n_regimes and 0 <= curr_regime < n_regimes:\n",
    "            transitions[prev_regime, curr_regime] += 1\n",
    "    \n",
    "    # Convert to probabilities (row-wise)\n",
    "    row_sums = transitions.sum(axis=1, keepdims=True)\n",
    "    # Avoid division by zero\n",
    "    row_sums[row_sums == 0] = 1\n",
    "    transition_matrix = transitions / row_sums\n",
    "    \n",
    "    # Calculate stationary distribution (Perron-Frobenius)\n",
    "    eigenvalues, eigenvectors = linalg.eig(transition_matrix.T)\n",
    "    # Find index of eigenvalue closest to 1\n",
    "    idx = np.argmin(np.abs(eigenvalues - 1.0))\n",
    "    # Extract corresponding eigenvector and normalize\n",
    "    stationary_dist = np.real(eigenvectors[:, idx] / np.sum(eigenvectors[:, idx]))\n",
    "    \n",
    "    return regime_df, transition_matrix, stationary_dist\n",
    "\n",
    "# Portfolio Optimization with Diagonalization\n",
    "def optimize_portfolio(returns_df, loadings_df, regime_info=None, min_weight=-0.5, max_weight=1.5):\n",
    "    \"\"\"\n",
    "    Create optimal portfolio weights using factor loadings and regime information\n",
    "    \n",
    "    Parameters:\n",
    "    - returns_df: DataFrame of asset returns\n",
    "    - loadings_df: Factor loadings from PCA/SVD\n",
    "    - regime_info: Current regime information\n",
    "    - min_weight/max_weight: Weight constraints for individual assets\n",
    "    \n",
    "    Returns:\n",
    "    - weights: Optimal portfolio weights\n",
    "    \"\"\"\n",
    "    n_assets = returns_df.shape[1]\n",
    "    \n",
    "    # Get asset names\n",
    "    assets = returns_df.columns\n",
    "    \n",
    "    # 1. Compute covariance matrix\n",
    "    cov_matrix = np.cov(returns_df.values.T)\n",
    "    \n",
    "    # 2. Efficient diagonalization for inverse\n",
    "    eigenvalues, eigenvectors = linalg.eigh(cov_matrix)\n",
    "    \n",
    "    # Replace any near-zero eigenvalues to avoid numerical instability\n",
    "    eigenvalues[eigenvalues < 1e-10] = 1e-10\n",
    "    \n",
    "    # Compute inverse covariance via diagonalization: C⁻¹ = V Λ⁻¹ V^T\n",
    "    inv_cov = eigenvectors @ np.diag(1/eigenvalues) @ eigenvectors.T\n",
    "    \n",
    "    # 3. Initialize with minimum variance portfolio\n",
    "    ones = np.ones(n_assets)\n",
    "    min_var_weights = inv_cov @ ones\n",
    "    min_var_weights = min_var_weights / np.sum(min_var_weights)\n",
    "    \n",
    "    # 4. Adjust weights based on factor loadings\n",
    "    # Get absolute loadings on first factor (market factor)\n",
    "    market_loadings = np.abs(loadings_df.iloc[:, 0].values)\n",
    "    \n",
    "    # Use second factor for sector rotation/style tilts\n",
    "    if loadings_df.shape[1] > 1:\n",
    "        style_loadings = loadings_df.iloc[:, 1].values\n",
    "    else:\n",
    "        style_loadings = np.zeros(n_assets)\n",
    "    \n",
    "    # 5. Regime adjustments (if provided)\n",
    "    regime_adjustment = np.ones(n_assets)\n",
    "    if regime_info is not None:\n",
    "        regime, transition_matrix, stationary_dist = regime_info\n",
    "        \n",
    "        # In high volatility regimes (2), reduce exposure to high-beta assets\n",
    "        if regime == 2:  # Crisis regime\n",
    "            # Reduce weights of high market factor exposure\n",
    "            regime_adjustment = 1 - 0.5 * market_loadings / np.max(market_loadings) if np.max(market_loadings) > 0 else np.ones(n_assets)\n",
    "        # In low volatility regimes (0), increase exposure to momentum\n",
    "        elif regime == 0:  # Calm regime\n",
    "            # Increase weights of positive style factor exposure\n",
    "            std_style = np.std(style_loadings)\n",
    "            if std_style > 0:\n",
    "                regime_adjustment = 1 + 0.3 * (style_loadings - np.mean(style_loadings)) / std_style\n",
    "    \n",
    "    # 6. Final weights combining minimum variance with factor tilts\n",
    "    final_weights = min_var_weights * regime_adjustment\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    if np.sum(np.abs(final_weights)) > 0:\n",
    "        final_weights = final_weights / np.sum(np.abs(final_weights))\n",
    "    \n",
    "    # Apply constraints\n",
    "    final_weights = np.clip(final_weights, min_weight, max_weight)\n",
    "    \n",
    "    # Normalize again after constraints\n",
    "    if np.sum(np.abs(final_weights)) > 0:\n",
    "        final_weights = final_weights / np.sum(np.abs(final_weights))\n",
    "    \n",
    "    # Return as Series\n",
    "    return pd.Series(final_weights, index=assets)\n",
    "\n",
    "# SVD Momentum Function\n",
    "def calculate_svd_momentum(returns_df, lookback=60, n_vals=3):\n",
    "    \"\"\"\n",
    "    Calculate momentum signals based on singular value trends\n",
    "    \n",
    "    Parameters:\n",
    "    - returns_df: DataFrame of asset returns\n",
    "    - lookback: Lookback period for SVD\n",
    "    - n_vals: Number of singular values to track\n",
    "    \n",
    "    Returns:\n",
    "    - momentum_df: DataFrame with SVD momentum scores for each asset\n",
    "    \"\"\"\n",
    "    # Initialize results DataFrame\n",
    "    momentum_df = pd.DataFrame(index=returns_df.columns, columns=['SVD_Momentum'])\n",
    "    \n",
    "    # Need sufficient history\n",
    "    if returns_df.shape[0] < lookback:\n",
    "        print(f\"Warning: Not enough data for SVD momentum ({returns_df.shape[0]} < {lookback})\")\n",
    "        return momentum_df\n",
    "    \n",
    "    # Get recent returns\n",
    "    recent_returns = returns_df.iloc[-lookback:].values\n",
    "    \n",
    "    # Calculate SVD\n",
    "    U, S, Vt = linalg.svd(recent_returns, full_matrices=False)\n",
    "    \n",
    "    # Calculate momentum scores: project each asset onto top singular vectors\n",
    "    # Higher score = more aligned with dominant market trends\n",
    "    momentum_scores = np.zeros(returns_df.shape[1])\n",
    "    \n",
    "    # Use only top n_vals singular vectors\n",
    "    for i in range(min(n_vals, len(S))):\n",
    "        # Weight by singular value (strength of the factor)\n",
    "        weight = S[i] / sum(S[:min(n_vals, len(S))])\n",
    "        # Get right singular vector (asset loadings on this factor)\n",
    "        # Note: Vt[i] gives loadings of all assets on the ith singular vector\n",
    "        momentum_scores += weight * Vt[i]\n",
    "    \n",
    "    # Normalize scores (avoid division by zero)\n",
    "    if np.std(momentum_scores) > 0:\n",
    "        momentum_scores = (momentum_scores - np.mean(momentum_scores)) / np.std(momentum_scores)\n",
    "    \n",
    "    # Store results\n",
    "    momentum_df['SVD_Momentum'] = momentum_scores\n",
    "    \n",
    "    return momentum_df\n",
    "\n",
    "# Eigen portfolio strategy\n",
    "def eigen_portfolio_strategy(ticker, date, lookback=120, holding_period=10):\n",
    "    \"\"\"\n",
    "    Multi-Factor Eigen Portfolio strategy \n",
    "    \n",
    "    Parameters:\n",
    "    - ticker: Target ticker for trading\n",
    "    - date: Current date\n",
    "    - lookback: Historical period for analysis\n",
    "    - holding_period: Position holding period\n",
    "    \n",
    "    Returns:\n",
    "    - Trade details dictionary or None if no trade\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Get universe data - we need price history for multiple assets\n",
    "        # Use a market ETF (SPY) plus sector ETFs for broader context\n",
    "        universe = [\n",
    "            'SPY',   # S&P 500\n",
    "            'XLK',   # Technology\n",
    "            'XLF',   # Financials \n",
    "            'XLE',   # Energy\n",
    "            'XLV',   # Healthcare\n",
    "            'XLP',   # Consumer Staples\n",
    "            'XLY',   # Consumer Discretionary\n",
    "            'XLI',   # Industrials\n",
    "            'XLB',   # Materials\n",
    "            'XLRE',  # Real Estate\n",
    "            'XLU',   # Utilities\n",
    "            'QQQ',   # NASDAQ 100\n",
    "            'IWM',   # Russell 2000\n",
    "            ticker   # Our target stock\n",
    "        ]\n",
    "        \n",
    "        # Calculate start date for data\n",
    "        start_date = date - timedelta(days=lookback * 2)\n",
    "        \n",
    "        # Get price data for universe\n",
    "        universe_data = {}\n",
    "        for symbol in universe:\n",
    "            data = get_historical_data(\n",
    "                symbol,\n",
    "                start_date=start_date,\n",
    "                end_date=date + timedelta(days=holding_period * 2),\n",
    "                interval='1d'\n",
    "            )\n",
    "            if not data.empty:\n",
    "                universe_data[symbol] = data\n",
    "        \n",
    "        # Need at least the target ticker plus some context\n",
    "        min_tickers = 5\n",
    "        if len(universe_data) < min_tickers or ticker not in universe_data:\n",
    "            print(f\"Not enough tickers with data (need {min_tickers}, got {len(universe_data)})\")\n",
    "            return None\n",
    "        \n",
    "        # 2. Calculate returns for analysis\n",
    "        returns_dict = {}\n",
    "        for symbol, data in universe_data.items():\n",
    "            returns_dict[symbol] = data['Close'].pct_change().fillna(0)\n",
    "        \n",
    "        # Create returns DataFrame\n",
    "        returns_df = pd.DataFrame(returns_dict)\n",
    "        \n",
    "        # Filter to lookback period\n",
    "        if len(returns_df) < lookback:\n",
    "            print(f\"Not enough return data (need {lookback}, got {len(returns_df)})\")\n",
    "            return None\n",
    "        \n",
    "        analysis_returns = returns_df.iloc[-lookback:]\n",
    "        \n",
    "        # 3. Extract factors using PCA & SVD\n",
    "        n_factors = min(3, len(universe_data) - 1)  # Use up to 3 factors\n",
    "        factors_df, loadings_df, eigenvalues = extract_factors(\n",
    "            analysis_returns, \n",
    "            n_factors=n_factors,\n",
    "            denoise=True\n",
    "        )\n",
    "        \n",
    "        # 4. Detect market regime\n",
    "        try:\n",
    "            regime_info = detect_market_regime(\n",
    "                analysis_returns,\n",
    "                n_regimes=3,\n",
    "                lookback=min(60, lookback // 2)\n",
    "            )\n",
    "            \n",
    "            if regime_info is not None:\n",
    "                regime_df, transition_matrix, stationary_dist = regime_info\n",
    "                # Get current regime (most recent)\n",
    "                current_regime = int(regime_df['Regime'].iloc[-1])\n",
    "            else:\n",
    "                current_regime = 1  # Default to middle regime\n",
    "                regime_info = None\n",
    "        except Exception as e:\n",
    "            print(f\"Regime detection error: {str(e)}\")\n",
    "            current_regime = 1  # Default to middle regime\n",
    "            regime_info = None\n",
    "        \n",
    "        # 5. Calculate SVD momentum\n",
    "        momentum_df = calculate_svd_momentum(\n",
    "            analysis_returns,\n",
    "            lookback=min(60, lookback // 2),\n",
    "            n_vals=2\n",
    "        )\n",
    "        \n",
    "        # 6. Generate portfolio weights\n",
    "        optimal_weights = optimize_portfolio(\n",
    "            analysis_returns,\n",
    "            loadings_df,\n",
    "            regime_info=(current_regime, None, None) if regime_info else None\n",
    "        )\n",
    "        \n",
    "        # 7. Get signal for target ticker\n",
    "        ticker_weight = optimal_weights.get(ticker, 0)\n",
    "        ticker_momentum = momentum_df.loc[ticker, 'SVD_Momentum']\n",
    "        ticker_loading = loadings_df.loc[ticker, 'Factor_1']  # Loading on market factor\n",
    "        \n",
    "        # Combine signals\n",
    "        # Weight: portfolio optimization result\n",
    "        # Momentum: SVD trend strength\n",
    "        # Loading: exposure to market factor\n",
    "        combined_signal = (\n",
    "            0.5 * np.sign(ticker_weight) * min(abs(ticker_weight * 3), 1) +\n",
    "            0.4 * np.sign(ticker_momentum) * min(abs(ticker_momentum), 1) +\n",
    "            0.1 * np.sign(-ticker_loading) * min(abs(ticker_loading * 2), 1)\n",
    "        )\n",
    "        \n",
    "        # Determine direction\n",
    "        print(f\"Ticker: {ticker}, Date: {date}, Signal: {combined_signal:.2f}\")\n",
    "\n",
    "        if combined_signal > 0.4:\n",
    "            direction = 'Long'\n",
    "        elif combined_signal < -0.4:\n",
    "            direction = 'Short'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # 8. Calculate entry and exit dates\n",
    "        entry_date = date\n",
    "        exit_date = entry_date\n",
    "        for _ in range(holding_period):\n",
    "            exit_date = get_next_trading_day(exit_date)\n",
    "        \n",
    "        # 9. Get prices for the trade\n",
    "        ticker_prices = universe_data[ticker]\n",
    "        entry_prices = ticker_prices[ticker_prices.index.date == entry_date.date()]\n",
    "        exit_prices = ticker_prices[ticker_prices.index.date == exit_date.date()]\n",
    "        \n",
    "        if entry_prices.empty or exit_prices.empty:\n",
    "            return None\n",
    "        \n",
    "        entry_price = entry_prices['Close'].iloc[0]\n",
    "        exit_price = exit_prices['Close'].iloc[0]\n",
    "        \n",
    "        # 10. Calculate return\n",
    "        if direction == 'Long':\n",
    "            ret = (exit_price - entry_price) / entry_price\n",
    "        else:  # Short\n",
    "            ret = (entry_price - exit_price) / entry_price\n",
    "        \n",
    "        # 11. Create trade details\n",
    "        trade = {\n",
    "            'Symbol': ticker,\n",
    "            'Strategy': 'Eigen-Portfolio',\n",
    "            'Direction': direction,\n",
    "            'Signal': float(combined_signal),\n",
    "            'Regime': int(current_regime),\n",
    "            'SVD_Momentum': float(ticker_momentum),\n",
    "            'Market_Loading': float(ticker_loading),\n",
    "            'Optimal_Weight': float(ticker_weight),\n",
    "            'Entry Date': entry_date.date(),\n",
    "            'Entry Price': float(entry_price),\n",
    "            'Exit Date': exit_date.date(),\n",
    "            'Exit Price': float(exit_price),\n",
    "            'Return': float(ret),\n",
    "            'Return %': float(ret * 100)\n",
    "        }\n",
    "        \n",
    "        return trade\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in strategy for {ticker}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Function to calculate profit factor\n",
    "def calculate_profit_factor(returns):\n",
    "    \"\"\"Calculate profit factor: sum of profits / sum of losses\"\"\"\n",
    "    profits = returns[returns > 0].sum()\n",
    "    losses = -returns[returns < 0].sum()\n",
    "    return profits / losses if losses > 0 else float('inf')\n",
    "\n",
    "# Run the backtest\n",
    "def run_backtest(tickers, start_date, end_date, lookback=120, holding_period=10, check_interval=5):\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_dt = pd.to_datetime(end_date)\n",
    "    \n",
    "    all_trades = []\n",
    "    \n",
    "    # Step through dates\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date <= end_dt:\n",
    "        # Only process if market is open\n",
    "        if is_market_open(current_date):\n",
    "            print(f\"Processing date: {current_date.date()}\")\n",
    "            \n",
    "            sampled_tickers = tickers\n",
    "            \n",
    "            for ticker in sampled_tickers:\n",
    "                trade = eigen_portfolio_strategy(\n",
    "                    ticker, \n",
    "                    current_date,\n",
    "                    lookback=lookback,\n",
    "                    holding_period=holding_period\n",
    "                )\n",
    "                \n",
    "                if trade:\n",
    "                    # Only include trades with entry in our backtest window\n",
    "                    entry_date = trade[\"Entry Date\"]\n",
    "                    if start_date.date() <= entry_date <= end_dt.date():\n",
    "                        all_trades.append(trade)\n",
    "            \n",
    "            # Advance check_interval trading days\n",
    "            for _ in range(check_interval):\n",
    "                current_date = get_next_trading_day(current_date)\n",
    "        else:\n",
    "            # Go to next day if market closed\n",
    "            current_date += timedelta(days=1)\n",
    "    \n",
    "    # Build a DataFrame & inspect\n",
    "    results = None\n",
    "    if all_trades:\n",
    "        results = pd.DataFrame(all_trades)\n",
    "        results.sort_values(\"Entry Date\", inplace=True)\n",
    "        print(\"\\n--- Sample Trades ---\")\n",
    "        print(results.head(10).to_string(index=False))\n",
    "    \n",
    "        # Simple performance summary\n",
    "        total_ret = results[\"Return\"].sum()\n",
    "        avg_ret = results[\"Return\"].mean()\n",
    "        win_rate = (results[\"Return\"] > 0).mean()\n",
    "    \n",
    "        print(f\"\\nTotal P&L:    {total_ret:.4f} ({total_ret*100:.2f}%)\")\n",
    "        print(f\"Avg per trade:{avg_ret:.4f} ({avg_ret*100:.2f}%)\")\n",
    "        print(f\"Win Rate:     {win_rate:.2%}\")\n",
    "    else:\n",
    "        print(\"No trades generated in this period.\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8edb170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing date: 2025-03-17\n",
      "Error in strategy for MSFT: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for NVDA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AAPL: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AMZN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for GOOGL: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for GOOG: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for META: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for TSLA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AVGO: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for CMCSA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for PEP: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for COST: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for NFLX: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for INTC: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for ADBE: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for TXN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for QCOM: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for ASML: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AMGN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for CSCO: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for SBUX: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for MRNA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for BIIB: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for LRCX: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for INTU: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for ILMN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for GILD: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for BKNG: 'Index' object has no attribute 'tzinfo'\n",
      "Processing date: 2025-03-24\n",
      "Error in strategy for MSFT: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for NVDA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AAPL: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AMZN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for GOOGL: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for GOOG: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for META: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for TSLA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AVGO: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for CMCSA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for PEP: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for COST: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for NFLX: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for INTC: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for ADBE: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for TXN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for QCOM: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for ASML: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for AMGN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for CSCO: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for SBUX: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for MRNA: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for BIIB: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for LRCX: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for INTU: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for ILMN: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for GILD: 'Index' object has no attribute 'tzinfo'\n",
      "Error in strategy for BKNG: 'Index' object has no attribute 'tzinfo'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas_market_calendars\\market_calendar.py:543\u001b[39m, in \u001b[36mMarketCalendar.holidays\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    542\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_holidays\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: 'NYSEExchangeCalendar' object has no attribute '_holidays'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m start_date = \u001b[33m\"\u001b[39m\u001b[33m2025-03-15\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m end_date   = \u001b[33m\"\u001b[39m\u001b[33m2025-05-15\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m results = \u001b[43mrun_backtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtickers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 569\u001b[39m, in \u001b[36mrun_backtest\u001b[39m\u001b[34m(tickers, start_date, end_date, lookback, holding_period, check_interval)\u001b[39m\n\u001b[32m    567\u001b[39m     \u001b[38;5;66;03m# Advance check_interval trading days\u001b[39;00m\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(check_interval):\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m         current_date = \u001b[43mget_next_trading_day\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    571\u001b[39m     \u001b[38;5;66;03m# Go to next day if market closed\u001b[39;00m\n\u001b[32m    572\u001b[39m     current_date += timedelta(days=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mget_next_trading_day\u001b[39m\u001b[34m(date)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     11\u001b[39m     d += timedelta(days=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_market_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     13\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mis_market_open\u001b[39m\u001b[34m(date)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_market_open\u001b[39m(date):\n\u001b[32m      4\u001b[39m     nyse = mcal.get_calendar(\u001b[33m'\u001b[39m\u001b[33mNYSE\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     sched = \u001b[43mnyse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sched.empty\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas_market_calendars\\market_calendar.py:701\u001b[39m, in \u001b[36mMarketCalendar.schedule\u001b[39m\u001b[34m(self, start_date, end_date, tz, start, end, force_special_times, market_times, interruptions)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (start_date <= end_date):\n\u001b[32m    699\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mstart_date must be before or equal to end_date.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m _all_days = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalid_days\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[38;5;66;03m# Setup all valid trading days and the requested market_times\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m market_times \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas_market_calendars\\calendars\\nyse.py:1326\u001b[39m, in \u001b[36mNYSEExchangeCalendar.valid_days\u001b[39m\u001b[34m(self, start_date, end_date, tz)\u001b[39m\n\u001b[32m   1324\u001b[39m \u001b[38;5;66;03m# Don't care about Saturdays. Call super.\u001b[39;00m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_date > saturday_end:\n\u001b[32m-> \u001b[39m\u001b[32m1326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalid_days\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1328\u001b[39m \u001b[38;5;66;03m# Full Date Range is pre 1952. Augment the Super call\u001b[39;00m\n\u001b[32m   1329\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m end_date <= saturday_end:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas_market_calendars\\market_calendar.py:561\u001b[39m, in \u001b[36mMarketCalendar.valid_days\u001b[39m\u001b[34m(self, start_date, end_date, tz)\u001b[39m\n\u001b[32m    552\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalid_days\u001b[39m(\u001b[38;5;28mself\u001b[39m, start_date, end_date, tz=\u001b[33m\"\u001b[39m\u001b[33mUTC\u001b[39m\u001b[33m\"\u001b[39m) -> pd.DatetimeIndex:\n\u001b[32m    553\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    554\u001b[39m \u001b[33;03m    Get a DatetimeIndex of valid open business days.\u001b[39;00m\n\u001b[32m    555\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    559\u001b[39m \u001b[33;03m    :return: DatetimeIndex of valid business days\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.date_range(start_date, end_date, freq=\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mholidays\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, tz=tz)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas_market_calendars\\market_calendar.py:545\u001b[39m, in \u001b[36mMarketCalendar.holidays\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    543\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._holidays\n\u001b[32m    544\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m     \u001b[38;5;28mself\u001b[39m._holidays = \u001b[43mCustomBusinessDay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m        \u001b[49m\u001b[43mholidays\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madhoc_holidays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcalendar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mregular_holidays\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweekmask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweekmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._holidays\n",
      "\u001b[36mFile \u001b[39m\u001b[32moffsets.pyx:4301\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.CustomBusinessDay.__init__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32moffsets.pyx:1706\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets.BusinessMixin._init_custom\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32moffsets.pyx:219\u001b[39m, in \u001b[36mpandas._libs.tslibs.offsets._get_calendar\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\tseries\\holiday.py:476\u001b[39m, in \u001b[36mAbstractHolidayCalendar.holidays\u001b[39m\u001b[34m(self, start, end, return_name)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# If we don't have a cache or the dates are outside the prior cache, we\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# get them again\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m start < \u001b[38;5;28mself\u001b[39m._cache[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m end > \u001b[38;5;28mself\u001b[39m._cache[\u001b[32m1\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     pre_holidays = \u001b[43m[\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrules\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pre_holidays:\n\u001b[32m    480\u001b[39m         \u001b[38;5;66;03m# error: Argument 1 to \"concat\" has incompatible type\u001b[39;00m\n\u001b[32m    481\u001b[39m         \u001b[38;5;66;03m# \"List[Union[Series, DatetimeIndex]]\"; expected\u001b[39;00m\n\u001b[32m    482\u001b[39m         \u001b[38;5;66;03m# \"Union[Iterable[DataFrame], Mapping[<nothing>, DataFrame]]\"\u001b[39;00m\n\u001b[32m    483\u001b[39m         holidays = concat(pre_holidays)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\tseries\\holiday.py:477\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    473\u001b[39m \u001b[38;5;66;03m# If we don't have a cache or the dates are outside the prior cache, we\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[38;5;66;03m# get them again\u001b[39;00m\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m start < \u001b[38;5;28mself\u001b[39m._cache[\u001b[32m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m end > \u001b[38;5;28mself\u001b[39m._cache[\u001b[32m1\u001b[39m]:\n\u001b[32m    476\u001b[39m     pre_holidays = [\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m         \u001b[43mrule\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_name\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m rule \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.rules\n\u001b[32m    478\u001b[39m     ]\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pre_holidays:\n\u001b[32m    480\u001b[39m         \u001b[38;5;66;03m# error: Argument 1 to \"concat\" has incompatible type\u001b[39;00m\n\u001b[32m    481\u001b[39m         \u001b[38;5;66;03m# \"List[Union[Series, DatetimeIndex]]\"; expected\u001b[39;00m\n\u001b[32m    482\u001b[39m         \u001b[38;5;66;03m# \"Union[Iterable[DataFrame], Mapping[<nothing>, DataFrame]]\"\u001b[39;00m\n\u001b[32m    483\u001b[39m         holidays = concat(pre_holidays)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\tseries\\holiday.py:282\u001b[39m, in \u001b[36mHoliday.dates\u001b[39m\u001b[34m(self, start_date, end_date, return_name)\u001b[39m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    280\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m dti\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m dates = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reference_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m holiday_dates = \u001b[38;5;28mself\u001b[39m._apply_rule(dates)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.days_of_week \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\tseries\\holiday.py:334\u001b[39m, in \u001b[36mHoliday._reference_dates\u001b[39m\u001b[34m(self, start_date, end_date)\u001b[39m\n\u001b[32m    330\u001b[39m reference_end_date = Timestamp(\n\u001b[32m    331\u001b[39m     datetime(end_date.year + \u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.month, \u001b[38;5;28mself\u001b[39m.day)\n\u001b[32m    332\u001b[39m )\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Don't process unnecessary holidays\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m dates = \u001b[43mdate_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_start_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreference_end_date\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43myear_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    338\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dates\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\datetimes.py:1008\u001b[39m, in \u001b[36mdate_range\u001b[39m\u001b[34m(start, end, periods, freq, tz, normalize, name, inclusive, unit, **kwargs)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m com.any_none(periods, start, end):\n\u001b[32m   1006\u001b[39m     freq = \u001b[33m\"\u001b[39m\u001b[33mD\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m dtarr = \u001b[43mDatetimeArray\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_generate_range\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m    \u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m=\u001b[49m\u001b[43mperiods\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclusive\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclusive\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DatetimeIndex._simple_new(dtarr, name=name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:468\u001b[39m, in \u001b[36mDatetimeArray._generate_range\u001b[39m\u001b[34m(cls, start, end, periods, freq, tz, normalize, ambiguous, nonexistent, inclusive, unit)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    465\u001b[39m     xdr = _generate_range(\n\u001b[32m    466\u001b[39m         start=start, end=end, periods=periods, offset=freq, unit=unit\n\u001b[32m    467\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     i8values = np.array(\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxdr\u001b[49m\u001b[43m]\u001b[49m, dtype=np.int64)\n\u001b[32m    470\u001b[39m endpoint_tz = start.tz \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m end.tz\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m endpoint_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:468\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    465\u001b[39m     xdr = _generate_range(\n\u001b[32m    466\u001b[39m         start=start, end=end, periods=periods, offset=freq, unit=unit\n\u001b[32m    467\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     i8values = np.array(\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxdr\u001b[49m\u001b[43m]\u001b[49m, dtype=np.int64)\n\u001b[32m    470\u001b[39m endpoint_tz = start.tz \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m end.tz\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m endpoint_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\Coding\\Trading\\venv\\Lib\\site-packages\\pandas\\core\\arrays\\datetimes.py:2801\u001b[39m, in \u001b[36m_generate_range\u001b[39m\u001b[34m(start, end, periods, offset, unit)\u001b[39m\n\u001b[32m   2798\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   2800\u001b[39m \u001b[38;5;66;03m# faster than cur + offset\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2801\u001b[39m next_date = \u001b[43moffset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcur\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2802\u001b[39m next_date = next_date.as_unit(unit)\n\u001b[32m   2803\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m next_date <= cur:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "tickers = [\n",
    "    # Mega‑caps (>$1 trillion)\n",
    "    \"MSFT\",  # Microsoft :contentReference[oaicite:0]{index=0}\n",
    "    \"NVDA\",  # Nvidia :contentReference[oaicite:1]{index=1}\n",
    "    \"AAPL\",  # Apple :contentReference[oaicite:2]{index=2}\n",
    "    \"AMZN\",  # Amazon :contentReference[oaicite:3]{index=3}\n",
    "    \"GOOGL\", # Alphabet Class A :contentReference[oaicite:4]{index=4}\n",
    "    \"GOOG\",  # Alphabet Class C :contentReference[oaicite:5]{index=5}\n",
    "    \"META\",  # Meta Platforms :contentReference[oaicite:6]{index=6}\n",
    "    \"TSLA\",  # Tesla :contentReference[oaicite:7]{index=7}\n",
    "\n",
    "    # Next‑tier big‑caps ($100 B+)\n",
    "    \"AVGO\",  # Broadcom :contentReference[oaicite:8]{index=8}\n",
    "    \"CMCSA\", # Comcast :contentReference[oaicite:9]{index=9}\n",
    "    \"PEP\",   # PepsiCo (Nasdaq‑listed) :contentReference[oaicite:10]{index=10}\n",
    "    \"COST\",  # Costco Wholesale :contentReference[oaicite:11]{index=11}\n",
    "    \"NFLX\",  # Netflix :contentReference[oaicite:12]{index=12}\n",
    "    \"INTC\",  # Intel :contentReference[oaicite:13]{index=13}\n",
    "    \"ADBE\",  # Adobe :contentReference[oaicite:14]{index=14}\n",
    "    \"TXN\",   # Texas Instruments :contentReference[oaicite:15]{index=15}\n",
    "    \"QCOM\",  # Qualcomm :contentReference[oaicite:16]{index=16}\n",
    "    \"ASML\",  # ASML Holding :contentReference[oaicite:17]{index=17}\n",
    "    \"AMGN\",  # Amgen :contentReference[oaicite:18]{index=18}\n",
    "\n",
    "    # Other large‑caps ($50 B+)\n",
    "    \"CSCO\",  # Cisco Systems :contentReference[oaicite:19]{index=19}\n",
    "    \"SBUX\",  # Starbucks :contentReference[oaicite:20]{index=20}\n",
    "    \"MRNA\",  # Moderna :contentReference[oaicite:21]{index=21}\n",
    "    \"BIIB\",  # Biogen :contentReference[oaicite:22]{index=22}\n",
    "    \"LRCX\",  # Lam Research :contentReference[oaicite:23]{index=23}\n",
    "    \"INTU\",  # Intuit :contentReference[oaicite:24]{index=24}\n",
    "    \"ILMN\",  # Illumina :contentReference[oaicite:25]{index=25}\n",
    "    \"GILD\",  # Gilead Sciences :contentReference[oaicite:26]{index=26}\n",
    "    \"BKNG\",  # Booking Holdings :contentReference[oaicite:27]{index=27}\n",
    "]\n",
    "\n",
    "start_date = \"2025-03-15\"\n",
    "end_date   = \"2025-05-15\"\n",
    "\n",
    "results = run_backtest(tickers, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f86dca3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mresults\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Collect metrics for all tickers\u001b[39;00m\n\u001b[32m      3\u001b[39m     all_metrics = []\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mset\u001b[39m(results[\u001b[33m'\u001b[39m\u001b[33mSymbol\u001b[39m\u001b[33m'\u001b[39m]):\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "if not results.empty:\n",
    "    # Collect metrics for all tickers\n",
    "    all_metrics = []\n",
    "    for ticker in set(results['Symbol']):\n",
    "        metrics = get_stock_metrics(ticker)\n",
    "        all_metrics.append(metrics)\n",
    "\n",
    "    # Create metrics dataframe\n",
    "    metrics_df = pd.DataFrame(all_metrics)\n",
    "\n",
    "    # Merge with aggregated results\n",
    "    symbol_perf = pd.merge(\n",
    "        results, \n",
    "        metrics_df,\n",
    "        on='Symbol'\n",
    "    )\n",
    "\n",
    "    cap_size_perf = symbol_perf.groupby('CapSize').agg({\n",
    "        'Return': ['count', 'mean', 'sum', 'std'],\n",
    "    })\n",
    "    cap_size_perf.columns = ['Count', 'Avg Return', 'Total Return', 'Std Dev']\n",
    "    cap_size_perf['Win Rate'] = symbol_perf.groupby('CapSize')['Return'].apply(\n",
    "        lambda x: (x > 0).mean()\n",
    "    )\n",
    "    cap_size_perf['Profit Factor'] = symbol_perf.groupby('CapSize')['Return'].apply(\n",
    "        lambda x: calculate_profit_factor(x.values)\n",
    "    )   \n",
    "    print(\"\\n--- Performance by Market Cap ---\")\n",
    "    print(cap_size_perf)\n",
    "\n",
    "    sector_perf = symbol_perf.groupby('Sector').agg({\n",
    "        'Return': ['count', 'mean', 'sum', 'std'],\n",
    "    })\n",
    "    sector_perf.columns = ['Count', 'Avg Return', 'Total Return', 'Std Dev']\n",
    "    sector_perf['Win Rate'] = symbol_perf.groupby('Sector')['Return'].apply(\n",
    "        lambda x: (x > 0).mean()\n",
    "    )\n",
    "    print(\"\\n--- Performance by Sector ---\")\n",
    "    print(sector_perf)\n",
    "\n",
    "    strategy_cap = symbol_perf.groupby(['Strategy', 'CapSize']).agg({\n",
    "        'Return': ['count', 'mean', 'sum']\n",
    "    })\n",
    "    strategy_cap.columns = ['Count', 'Avg Return', 'Total Return']\n",
    "    print(\"\\n--- Strategy by Market Cap ---\")\n",
    "    print(strategy_cap)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(\n",
    "        symbol_perf['Beta'], \n",
    "        symbol_perf['Return'],\n",
    "        alpha=0.5, \n",
    "        c=symbol_perf['Strategy'].map({'Pre-Earnings': 'blue', 'Post-Earnings': 'red'})\n",
    "    )\n",
    "    plt.axhline(y=0, color='black', linestyle='--')\n",
    "    plt.xlabel('Beta')\n",
    "    plt.ylabel('Return')\n",
    "    plt.title('Returns vs Beta by Strategy')\n",
    "    plt.legend(['', '', 'Pre-Earnings', 'Post-Earnings'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "    # -------------------------------\n",
    "    # Performance Validity Checks\n",
    "    # -------------------------------\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nPerformance Validity Analysis\\n\" + \"=\"*40)\n",
    "\n",
    "    # Convert to datetime for sorting (if not already)\n",
    "    if 'Entry Date' in results.columns and not pd.api.types.is_datetime64_dtype(results['Entry Date']):\n",
    "        results['Entry Date'] = pd.to_datetime(results['Entry Date'])\n",
    "    \n",
    "    # Sort results by entry date\n",
    "    results.sort_values('Entry Date', inplace=True)\n",
    "\n",
    "    # Create trade sequence index\n",
    "    results['Trade Number'] = range(1, len(results)+1)\n",
    "\n",
    "    # 1. Gap-Free Cumulative Returns Timeline\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cumulative_log_returns = np.log1p(results['Return']).cumsum()\n",
    "\n",
    "    # Plot by trade sequence instead of calendar time\n",
    "    plt.plot(results['Trade Number'], cumulative_log_returns, \n",
    "            color='red', linewidth=1.5)\n",
    "    plt.title('Cumulative Log Returns (Trade Sequence)')\n",
    "    plt.xlabel('Trade Number')\n",
    "    plt.ylabel('Cumulative Log Returns')\n",
    "    plt.grid(True)\n",
    "    plt.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "    rolling_window = min(30, len(results) // 2) if len(results) > 4 else 2\n",
    "    rolling_mean = cumulative_log_returns.rolling(rolling_window).mean()\n",
    "    plt.plot(results['Trade Number'], rolling_mean,\n",
    "            color='blue', linestyle='--', linewidth=1,\n",
    "            label=f'{rolling_window}-Trade Rolling Mean')\n",
    "\n",
    "    rolling_std = cumulative_log_returns.rolling(rolling_window).std()\n",
    "    plt.fill_between(results['Trade Number'],\n",
    "                    rolling_mean - 2*rolling_std,\n",
    "                    rolling_mean + 2*rolling_std,\n",
    "                    color='yellow', alpha=0.5,\n",
    "                    label='2σ Volatility Bands')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Enhanced Drawdown Analysis\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    cumulative_returns = np.exp(cumulative_log_returns) - 1\n",
    "    peak = cumulative_returns.expanding(min_periods=1).max()\n",
    "    drawdown = (cumulative_returns - peak)/peak\n",
    "\n",
    "    # Find maximum drawdown details\n",
    "    max_dd = drawdown.min()\n",
    "    max_dd_end_idx = drawdown.idxmin()\n",
    "    max_dd_start_idx = drawdown[:max_dd_end_idx].idxmax()\n",
    "\n",
    "    # Get actual datetime values from results\n",
    "    max_dd_start_date = results.loc[max_dd_start_idx, 'Entry Date']\n",
    "    max_dd_end_date = results.loc[max_dd_end_idx, 'Entry Date']\n",
    "\n",
    "    # Calculate duration in days\n",
    "    if isinstance(max_dd_start_date, pd.Timestamp) and isinstance(max_dd_end_date, pd.Timestamp):\n",
    "        max_dd_duration = (max_dd_end_date - max_dd_start_date).days\n",
    "    else:\n",
    "        max_dd_duration = (pd.to_datetime(max_dd_end_date) - pd.to_datetime(max_dd_start_date)).days\n",
    "\n",
    "    # Plot by trade sequence\n",
    "    plt.plot(results['Trade Number'], drawdown, \n",
    "            color='darkred', linewidth=1.5)\n",
    "    plt.title(f'Strategy Drawdown (Max: {max_dd:.2%}, Duration: {max_dd_duration:.1f} days)')\n",
    "    plt.xlabel('Trade Number')\n",
    "    plt.ylabel('Drawdown')\n",
    "    plt.axhline(0, color='black', linestyle='--')\n",
    "\n",
    "    max_dd_trade_num = results.loc[max_dd_end_idx, 'Trade Number']\n",
    "    prev_trade_num = results.loc[max_dd_start_idx, 'Trade Number']\n",
    "\n",
    "    plt.annotate(f'Max Drawdown: {max_dd:.2%}\\nDuration: {max_dd_duration:.1f} days',\n",
    "            xy=(max_dd_trade_num, max_dd),\n",
    "            xytext=(max_dd_trade_num - (max_dd_trade_num - prev_trade_num)/2, \n",
    "                    max_dd*1.5),\n",
    "            arrowprops=dict(facecolor='black', arrowstyle='->'),\n",
    "            bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='black'))\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def perform_profit_factor_permutation_test(returns, n_permutations=10000):\n",
    "    \"\"\"\n",
    "    Perform permutation test to evaluate statistical significance of profit factor\n",
    "    \n",
    "    Parameters:\n",
    "    - returns: Array of trade returns\n",
    "    - n_permutations: Number of permutations for the test\n",
    "    \n",
    "    Returns:\n",
    "    - p-value of the observed profit factor\n",
    "    \"\"\"\n",
    "    # Calculate the observed profit factor\n",
    "    observed_pf = calculate_profit_factor(returns)\n",
    "    \n",
    "    # Initialize array to store permutation results\n",
    "    permutation_pfs = np.zeros(n_permutations)\n",
    "    \n",
    "    # Perform permutation test by randomly flipping signs of returns\n",
    "    np.random.seed(69)\n",
    "    for i in range(n_permutations):\n",
    "        # Generate random signs (-1 or 1) for each return\n",
    "        random_signs = np.random.choice([-1, 1], size=len(returns))\n",
    "        # Apply random signs to returns\n",
    "        permuted_returns = returns * random_signs\n",
    "        # Calculate profit factor for permuted returns\n",
    "        permutation_pfs[i] = calculate_profit_factor(permuted_returns)\n",
    "    \n",
    "    # Calculate p-value: proportion of permuted PFs >= observed PF\n",
    "    p_value = np.mean(permutation_pfs >= observed_pf)\n",
    "    \n",
    "    # Plot histogram of permutation results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(permutation_pfs, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.axvline(observed_pf, color='red', linestyle='dashed', linewidth=2, label=f'Observed PF = {observed_pf:.2f}')\n",
    "    plt.title(f'Profit Factor Permutation Test (p-value = {p_value:.4f})')\n",
    "    plt.xlabel('Profit Factor')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary of permutation test\n",
    "    print(\"\\n=== Profit Factor Permutation Test ===\")\n",
    "    print(f\"Observed Profit Factor: {observed_pf:.4f}\")\n",
    "    print(f\"Mean Permutation Profit Factor: {np.mean(permutation_pfs):.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: The observed profit factor is statistically significant (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"Result: The observed profit factor is NOT statistically significant (p >= 0.05)\")\n",
    "    \n",
    "    return p_value\n",
    "\n",
    "# Only run if results exist\n",
    "if not results.empty:\n",
    "    # Extract returns array\n",
    "    returns_array = results['Return'].values\n",
    "\n",
    "    # Run permutation test (e.g., 10,000 permutations)\n",
    "    p_value = perform_profit_factor_permutation_test(returns_array, n_permutations=10000)\n",
    "\n",
    "    # Display p-value\n",
    "    print(f\"\\nProfit Factor Permutation Test p-value: {p_value:.4f}\")\n",
    "    if p_value < 0.05:\n",
    "        print(\"Result: Statistically significant profit factor (p < 0.05)\")\n",
    "    else:\n",
    "        print(\"Result: Profit factor not statistically significant (p >= 0.05)\")\n",
    "\n",
    "    # -------------------------------\n",
    "    # Return Distribution\n",
    "    # -------------------------------\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nReturn Distribution\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Return Statistics\n",
    "    print(\"\\n--- Return Statistics ---\")\n",
    "    stats_df = results['Return'].describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9, 0.99])\n",
    "    print(stats_df)\n",
    "    \n",
    "    # Worst Outcomes Table\n",
    "    print(\"\\n--- Worst Outcomes ---\")\n",
    "    worst_percentiles = [0.05, 0.01, 0.001]\n",
    "    worst_returns = []\n",
    "    worst_labels = []\n",
    "    \n",
    "    for p in worst_percentiles:\n",
    "        if len(results) >= int(1/p):  # Only calculate if we have enough data\n",
    "            worst_returns.append(results['Return'].quantile(p))\n",
    "            worst_labels.append(f'Worst {p*100:.2f}%')\n",
    "    \n",
    "    if worst_returns:\n",
    "        worst_df = pd.DataFrame([worst_returns], columns=worst_labels)\n",
    "        print(worst_df)\n",
    "    \n",
    "    # Histograms for Return Distributions\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.hist(results['Return'], bins=min(100, len(results)//2), color='skyblue', edgecolor='black')\n",
    "    plt.title('Unzoomed Return Distribution')\n",
    "    plt.xlabel('Return per Trade')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    if len(results) >= 100:  # Only create zoomed view if enough data\n",
    "        zoomed_data = results[results['Return'] > results['Return'].quantile(0.01)]\n",
    "        plt.hist(zoomed_data['Return'], bins=min(50, len(zoomed_data)//2), color='lightgreen', edgecolor='black')\n",
    "        plt.title('Zoomed Return Distribution (Worst 1% Removed)')\n",
    "    else:\n",
    "        plt.hist(results['Return'], bins=min(50, len(results)//2), color='lightgreen', edgecolor='black')\n",
    "        plt.title('Return Distribution')\n",
    "    plt.xlabel('Return per Trade')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Strategy Analysis\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nStrategy Analysis\\n\" + \"=\"*40)\n",
    "    \n",
    "    # Performance by strategy type\n",
    "    if 'Strategy' in results.columns:\n",
    "        strategy_perf = results.groupby('Strategy').agg({\n",
    "            'Return': ['count', 'mean', 'sum', 'std'],\n",
    "        })\n",
    "        \n",
    "        strategy_perf.columns = ['Count', 'Avg Return', 'Total Return', 'Std Dev']\n",
    "        strategy_perf['Win Rate'] = results.groupby('Strategy')['Return'].apply(\n",
    "            lambda x: (x > 0).mean()\n",
    "        )\n",
    "        strategy_perf['Profit Factor'] = results.groupby('Strategy')['Return'].apply(\n",
    "            lambda x: calculate_profit_factor(x.values)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Performance by Strategy ---\")\n",
    "        print(strategy_perf)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Bar chart for count\n",
    "        strategy_perf['Count'].plot(kind='bar', color='skyblue', ax=ax, position=1, width=0.3)\n",
    "        ax.set_ylabel('Number of Trades', color='skyblue')\n",
    "        ax.tick_params(axis='y', labelcolor='skyblue')\n",
    "        \n",
    "        # Secondary axis for return\n",
    "        ax2 = ax.twinx()\n",
    "        strategy_perf['Avg Return'].plot(kind='bar', color='green', ax=ax2, position=0, width=0.3)\n",
    "        ax2.set_ylabel('Average Return', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "        \n",
    "        plt.title('Strategy Performance Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Performance by direction (Long/Short)\n",
    "    if 'Direction' in results.columns:\n",
    "        dir_perf = results.groupby('Direction').agg({\n",
    "            'Return': ['count', 'mean', 'sum', 'std'],\n",
    "        })\n",
    "        \n",
    "        dir_perf.columns = ['Count', 'Avg Return', 'Total Return', 'Std Dev']\n",
    "        dir_perf['Win Rate'] = results.groupby('Direction')['Return'].apply(\n",
    "            lambda x: (x > 0).mean()\n",
    "        )\n",
    "        dir_perf['Profit Factor'] = results.groupby('Direction')['Return'].apply(\n",
    "            lambda x: calculate_profit_factor(x.values)\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Performance by Direction ---\")\n",
    "        print(dir_perf)\n",
    "        \n",
    "        # Bar chart\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Bar chart for count\n",
    "        dir_perf['Count'].plot(kind='bar', color='skyblue', ax=ax, position=1, width=0.3)\n",
    "        ax.set_ylabel('Number of Trades', color='skyblue')\n",
    "        ax.tick_params(axis='y', labelcolor='skyblue')\n",
    "        \n",
    "        # Secondary axis for return\n",
    "        ax2 = ax.twinx()\n",
    "        dir_perf['Avg Return'].plot(kind='bar', color='green', ax=ax2, position=0, width=0.3)\n",
    "        ax2.set_ylabel('Average Return', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "        \n",
    "        plt.title('Long vs Short Performance')\n",
    "        plt.xticks(rotation=0)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Kelly Criterion Analysis\n",
    "    # -------------------------------\n",
    "    print(\"\\n\" + \"=\"*40 + \"\\nKelly Criterion\\n\" + \"=\"*40)\n",
    "    \n",
    "    def monte_carlo_kelly(returns, kelly_fraction=1.0, n_sims=10000):\n",
    "        np.random.seed(69)\n",
    "        all_terminal = []\n",
    "        for _ in range(n_sims):\n",
    "            sampled_returns = np.random.choice(returns, size=len(returns), replace=True)\n",
    "            capital = 1000\n",
    "            for ret in sampled_returns:\n",
    "                capital *= (1 + kelly_fraction * ret)\n",
    "            all_terminal.append(capital)\n",
    "        return pd.Series(all_terminal)\n",
    "    \n",
    "    def plot_multiple_pnl_paths(returns, kelly_fraction=1.0, num_paths=100, label=''):\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        for _ in range(num_paths):\n",
    "            path = np.cumprod(1 + kelly_fraction * np.random.choice(returns, size=len(returns), replace=True))\n",
    "            plt.plot(path, lw=1, alpha=0.3)  # thin lines with transparency\n",
    "        plt.title(f'Multiple PnL Paths ({label}, {int(kelly_fraction*100)}% Kelly)')\n",
    "        plt.xlabel('Trade Number')\n",
    "        plt.ylabel('Normalized Capital')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    # Run Kelly simulations with all data (no time-based segmentation since we have limited data)\n",
    "    returns = results['Return'].values\n",
    "    \n",
    "    if len(returns) >= 10:  # Only run if we have enough trades\n",
    "        print(\"\\n--- Full Dataset Kelly Simulations ---\")\n",
    "        \n",
    "        for fraction in [1.0, 0.5, 0.3]:\n",
    "            terminal_pnl = monte_carlo_kelly(returns, kelly_fraction=fraction, n_sims=10000)\n",
    "            \n",
    "            # Stats Table for Terminal PnL\n",
    "            stats_table = terminal_pnl.describe(percentiles=[0.01, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99])\n",
    "            print(f\"\\nKelly Fraction: {int(fraction*100)}%\")\n",
    "            print(stats_table)\n",
    "            \n",
    "            # Histogram of Terminal Capital distribution\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.hist(terminal_pnl, bins=50, alpha=0.7, color='magenta', edgecolor='black')\n",
    "            plt.title(f'Terminal PnL Distribution (Full Dataset, {int(fraction*100)}% Kelly)')\n",
    "            plt.xlabel('Terminal Capital')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        \n",
    "            # Plot Multiple Simulation Paths (PnL Curves)\n",
    "            plot_multiple_pnl_paths(returns, kelly_fraction=fraction, num_paths=50, label='Full Dataset')\n",
    "    \n",
    "    # -------------------------------\n",
    "    # Symbol Analysis\n",
    "    # -------------------------------\n",
    "    if 'Symbol' in results.columns:\n",
    "        print(\"\\n\" + \"=\"*40 + \"\\nSymbol Analysis\\n\" + \"=\"*40)\n",
    "        \n",
    "        symbol_perf = results.groupby('Symbol').agg({\n",
    "            'Return': ['count', 'mean', 'sum', 'std'],\n",
    "        })\n",
    "        \n",
    "        symbol_perf.columns = ['Count', 'Avg Return', 'Total Return', 'Std Dev']\n",
    "        symbol_perf['Win Rate'] = results.groupby('Symbol')['Return'].apply(\n",
    "            lambda x: (x > 0).mean()\n",
    "        )\n",
    "        symbol_perf['Profit Factor'] = results.groupby('Symbol')['Return'].apply(\n",
    "            lambda x: calculate_profit_factor(x.values) if (x < 0).any() else float('inf')\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- Performance by Symbol ---\")\n",
    "        print(symbol_perf)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = plt.gca()\n",
    "        \n",
    "        # Bar chart for count\n",
    "        symbol_perf['Count'].plot(kind='bar', color='skyblue', ax=ax, position=1, width=0.3)\n",
    "        ax.set_ylabel('Number of Trades', color='skyblue')\n",
    "        ax.tick_params(axis='y', labelcolor='skyblue')\n",
    "        \n",
    "        # Secondary axis for return\n",
    "        ax2 = ax.twinx()\n",
    "        symbol_perf['Avg Return'].plot(kind='bar', color='green', ax=ax2, position=0, width=0.3)\n",
    "        ax2.set_ylabel('Average Return', color='green')\n",
    "        ax2.tick_params(axis='y', labelcolor='green')\n",
    "        \n",
    "        plt.title('Symbol Performance Comparison')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"No trades found with the given strategy.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
